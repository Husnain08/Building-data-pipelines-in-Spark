{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "engaged-paradise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:38:44.878248Z",
     "start_time": "2021-04-18T10:38:44.755525Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parental-tomorrow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:38:47.761798Z",
     "start_time": "2021-04-18T10:38:46.517258Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accessible-browser",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:38:47.771718Z",
     "start_time": "2021-04-18T10:38:47.762740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.context.SparkContext'>\n",
      "<property object at 0x0000027715422B38>\n"
     ]
    }
   ],
   "source": [
    "# Verify SparkContext\n",
    "from pyspark import SparkContext as sc\n",
    "\n",
    "print(sc)\n",
    "\n",
    "# Print Spark version\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "diagnostic-worthy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:39:07.580501Z",
     "start_time": "2021-04-18T10:38:47.773712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000027715661EC8>\n"
     ]
    }
   ],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create my_spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Print my_spark\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-sister",
   "metadata": {},
   "source": [
    "# Introduction to Big Data analysis with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "popular-kazakhstan",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:40:09.638517Z",
     "start_time": "2021-04-18T10:40:09.622559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "congressional-sequence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:40:09.920950Z",
     "start_time": "2021-04-18T10:40:09.914938Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"miniProject\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "static-success",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:40:14.248694Z",
     "start_time": "2021-04-18T10:40:10.349590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10],4).map(lambda x: x**2).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-stroke",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T19:20:52.739518Z",
     "start_time": "2021-04-10T19:20:52.733499Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "funky-concert",
   "metadata": {},
   "source": [
    "# Abstracting Data with RDDs\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "extraordinary-maldives",
   "metadata": {},
   "source": [
    "How to create a RDD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "current-enhancement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T15:07:12.911944Z",
     "start_time": "2021-04-11T15:07:12.372673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of RDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Create an RDD from a list of words\n",
    "# Method # 1 (Using sparkcontext's parallelize method)\n",
    "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
    "\n",
    "# Print out the type of the created object\n",
    "print(\"The type of RDD is\", type(RDD))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "destroyed-characterization",
   "metadata": {},
   "source": [
    "Method # 2. RDDs from External Datasets\n",
    "\n",
    "PySpark can easily create RDDs from files that are stored in external storage devices such as HDFS (Hadoop Distributed File System), Amazon S3 buckets, etc. However, the most common method of creating RDD's is from files stored in your local file system. This method takes a file path and reads it as a collection of lines. In this exercise, you'll create an RDD from the file path (file_path) with the file name README.md which is already available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "velvet-updating",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T15:11:59.896956Z",
     "start_time": "2021-04-11T15:11:58.158696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file_path is README.md\n",
      "The file type of fileRDD is <class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# Print the file_path\n",
    "file_path = 'README.md'\n",
    "print(\"The file_path is\", file_path)\n",
    "\n",
    "# Create a fileRDD from file_path\n",
    "fileRDD = sc.textFile(file_path)\n",
    "\n",
    "# Check the type of fileRDD\n",
    "print(\"The file type of fileRDD is\", type(fileRDD))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "interior-logging",
   "metadata": {},
   "source": [
    "Partitions in your data\n",
    "SparkContext's textFile() method takes an optional second argument called minPartitions for specifying the minimum number of partitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "personalized-meditation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T15:12:03.857256Z",
     "start_time": "2021-04-11T15:12:03.391715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in fileRDD is 2\n",
      "Number of partitions in fileRDD_part is 5\n"
     ]
    }
   ],
   "source": [
    "# Check the number of partitions in fileRDD\n",
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
    "\n",
    "# Create a fileRDD_part from file_path with 5 partitions\n",
    "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\n",
    "\n",
    "# Check the number of partitions in fileRDD_part\n",
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "improved-execution",
   "metadata": {},
   "source": [
    "Basic RDD Transformations and Actions in PySpark\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "artistic-content",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T15:14:52.916956Z",
     "start_time": "2021-04-11T15:14:52.911908Z"
    }
   },
   "source": [
    "Map and Collect\n",
    "The main method by which you can manipulate data in PySpark is using map(). The map() transformation takes in a function and applies it to each element in the RDD. It can be used to do any number of things, from fetching the website associated with each URL in our collection to just squaring the numbers. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "specific-quality",
   "metadata": {},
   "source": [
    "Basic Actions in Spark are:\n",
    "1. Collect()\n",
    "2. take(n)\n",
    "3. first()\n",
    "4. count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "enhanced-trash",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T15:22:34.265389Z",
     "start_time": "2021-04-11T15:22:34.246405Z"
    }
   },
   "outputs": [],
   "source": [
    "numbRDD = sc.parallelize([1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "loaded-camcorder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T15:22:47.012419Z",
     "start_time": "2021-04-11T15:22:34.470826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8\n",
      "27\n",
      "64\n",
      "125\n",
      "216\n",
      "343\n",
      "512\n",
      "729\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = numbRDD.map(lambda x: x**3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "    print(numb)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ultimate-fancy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T15:22:57.247457Z",
     "start_time": "2021-04-11T15:22:57.241520Z"
    }
   },
   "source": [
    "Filter and Count\n",
    "The RDD transformation filter() returns a new RDD containing only the elements that satisfy a particular function. It is useful for filtering large datasets based on a keyword. For this exercise, you'll filter out lines containing keyword Spark from fileRDD RDD which consists of lines of text from the README.md file. Next, you'll count the total number of lines containing the keyword Spark and finally print the first 4 lines of the filtered RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "synthetic-class",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T15:29:00.161118Z",
     "start_time": "2021-04-11T15:28:57.828672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of lines with the keyword Spark is 4\n",
      "# Basic Data Preprocessing and feature engineering for machine learning using Spark\n",
      "This project deals with basic data preprocessing using Spark and we build a complete end to end machine learning pipeline using Spark. We also explore how to work with SQL in PySpark and how to work with dataframes in Spark.\n",
      "1. Spark\n",
      "For Installing Spark, I recommend using this tutorial\n"
     ]
    }
   ],
   "source": [
    "# Filter the fileRDD to select lines with Spark keyword\n",
    "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\n",
    "\n",
    "# How many lines are there in fileRDD?\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "# Print the first four lines of fileRDD\n",
    "for line in fileRDD_filter.take(4): \n",
    "    print(line)\n",
    "# Note that the filter() operation does not mutate the existing fileRDD. Instead, it returns a pointer to an entirely new RDD."
   ]
  },
  {
   "cell_type": "raw",
   "id": "charitable-seven",
   "metadata": {},
   "source": [
    "Pair RDDs in PySpark\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "oriental-overhead",
   "metadata": {},
   "source": [
    "ReduceBykey and Collect\n",
    "One of the most popular pair RDD transformations is reduceByKey() which operates on key, value (k,v) pairs and merges the values for each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "catholic-duration",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:17:16.640407Z",
     "start_time": "2021-04-11T19:17:01.449403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 1 has 2 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 4 has 5 Counts\n"
     ]
    }
   ],
   "source": [
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "focused-stations",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T19:19:19.829679Z",
     "start_time": "2021-04-11T19:19:03.042139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 4 has 5 Counts\n",
      "Key 3 has 10 Counts\n",
      "Key 1 has 2 Counts\n"
     ]
    }
   ],
   "source": [
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "appropriate-clear",
   "metadata": {},
   "source": [
    "CountingBykeys\n",
    "For many datasets, it is important to count the number of keys in a key/value dataset. For example, counting the number of countries where the product was sold or to show the most popular baby names. In this simple exercise, you'll use the Rdd pair RDD that you created earlier and count the number of unique keys in that pair RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "textile-regular",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T21:08:18.546092Z",
     "start_time": "2021-04-11T21:08:08.623171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of total is <class 'collections.defaultdict'>\n",
      "key 1 has 1 counts\n",
      "key 3 has 2 counts\n",
      "key 4 has 1 counts\n"
     ]
    }
   ],
   "source": [
    "# Transform the rdd with countByKey()\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "# What is the type of total?\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "    print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "average-coupon",
   "metadata": {},
   "source": [
    "Create a base RDD and transform it\n",
    "The volume of unstructured data (log lines, images, binary files) in existence is growing dramatically, and PySpark is an excellent framework for analyzing this type of data through RDDs. In this 3 part exercise, you will write code that calculates the most common words from Complete Works of William Shakespeare.\n",
    "\n",
    "Here are the brief steps for writing the word counting program:\n",
    "\n",
    "Create a base RDD from Complete_Shakespeare.txt file.\n",
    "Use RDD transformation to create a long list of words from each element of the base RDD.\n",
    "Remove stop words from your data.\n",
    "Create pair RDD where each element is a pair tuple of ('w', 1)\n",
    "Group the elements of the pair RDD by key (word) and add up their values.\n",
    "Swap the keys (word) and values (counts) so that keys is count and value is the word.\n",
    "Finally, sort the RDD by descending order and print the 10 most frequent words and their frequencies.\n",
    "In this first exercise, you'll create a base RDD from Complete_Shakespeare.txt file and transform it to create a long list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "touched-cement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T21:10:38.252685Z",
     "start_time": "2021-04-11T21:10:38.248695Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"spark_bigdatafundamentals/Complete_Shakespeare.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "running-occasion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T21:12:41.364603Z",
     "start_time": "2021-04-11T21:12:39.354303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in splitRDD: 194074\n"
     ]
    }
   ],
   "source": [
    "# Create a baseRDD from the file path\n",
    "baseRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the lines of baseRDD into words\n",
    "splitRDD = baseRDD.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "# Count the total number of words\n",
    "print(\"Total number of words in splitRDD:\", splitRDD.count())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sticky-alfred",
   "metadata": {},
   "source": [
    "Remove stop words and reduce the dataset\n",
    "After splitting the lines in the file into a long list of words using flatMap() transformation, in the next step, you'll remove stop words from your data. Stop words are common words that are often uninteresting. For example \"I\", \"the\", \"a\" etc., are stop words. You can remove many obvious stop words with a list of your own. But for this exercise, you will just remove the stop words from a curated list stop_words provided to you in your environment.\n",
    "\n",
    "After removing stop words, you'll next create a pair RDD where each element is a pair tuple (k, v) where k is the key and v is the value. In this example, pair RDD is composed of (w, 1) where w is for each word in the RDD and 1 is a number. Finally, you'll combine the values with the same key from the pair RDD using reduceByKey() operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "double-avatar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T21:13:56.463567Z",
     "start_time": "2021-04-11T21:13:56.297017Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', 'don', 'should', 'now']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "smoking-celebration",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T21:16:15.448462Z",
     "start_time": "2021-04-11T21:16:15.390961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the words in lower case and remove stop words from stop_words\n",
    "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
    "\n",
    "# Create a tuple of the word and 1 \n",
    "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
    "\n",
    "# Count of the number of occurences of each word\n",
    "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "engaged-wallet",
   "metadata": {},
   "source": [
    "Print word frequencies\n",
    "After combining the values (counts) with the same key (word), you'll print the word frequencies using the take(N) action. You could have used the collect() action but as a best practice, it is not recommended as collect() returns all the elements from your RDD. You'll use take(N) instead, to return N elements from your RDD.\n",
    "\n",
    "What if we want to return the top 10 words? For this first, you'll need to swap the key (word) and values (counts) so that keys is count and value is the word. After you swap the key and value in the tuple, you'll sort the pair RDD based on the key (count) and print the top 10 words in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "elect-problem",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T21:22:58.185493Z",
     "start_time": "2021-04-11T21:22:53.153722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 9)\n",
      "('EBook', 1)\n",
      "('Shakespeare', 12)\n",
      "('', 65498)\n",
      "('use', 38)\n",
      "('anyone', 1)\n",
      "('anywhere', 1)\n",
      "('restrictions', 1)\n",
      "('whatsoever.', 1)\n",
      "('may', 162)\n",
      " has 65498 counts\n",
      "thou has 650 counts\n",
      "thy has 574 counts\n",
      "shall has 393 counts\n",
      "would has 311 counts\n",
      "good has 295 counts\n",
      "thee has 286 counts\n",
      "love has 273 counts\n",
      "Enter has 269 counts\n",
      "th' has 254 counts\n"
     ]
    }
   ],
   "source": [
    "# Display the first 10 words and their frequencies\n",
    "for word in resultRDD.take(10):\n",
    "\tprint(word)\n",
    "\n",
    "# Swap the keys and values \n",
    "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Sort the keys in descending order\n",
    "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words and their frequencies\n",
    "for word in resultRDD_swap_sort.take(10):\n",
    "\tprint(\"{} has {} counts\". format(word[1], word[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-sound",
   "metadata": {},
   "source": [
    "# PySpark SQL & DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-commerce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T19:48:39.151045Z",
     "start_time": "2021-04-13T19:48:36.955878Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "sporting-little",
   "metadata": {},
   "source": [
    "RDD to DataFrame\n",
    "Similar to RDDs, DataFrames are immutable and distributed data structures in Spark. Even though RDDs are a fundamental data structure in Spark, working with data in DataFrame is easier than RDD most of the time and so understanding of how to convert RDD to DataFrame is necessary.\n",
    "\n",
    "In this exercise, you'll first make an RDD using the sample_list which contains the list of tuples ('Mona',20), ('Jennifer',34),('John',20), ('Jim',26) with each tuple contains the name of the person and their age. Next, you'll create a DataFrame using the RDD and the schema (which is the list of 'Name' and 'Age') and finally confirm the output as PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "interracial-donna",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T19:59:33.766960Z",
     "start_time": "2021-04-13T19:59:09.140875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of names_df is <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Create a list of tuples\n",
    "sample_list = [('Mona',20), ('Jennifer',34), ('John',20), ('Jim',26)]\n",
    "\n",
    "# Create a RDD from the list\n",
    "rdd = sc.parallelize(sample_list)\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n",
    "\n",
    "# Check the type of names_df\n",
    "print(\"The type of names_df is\", type(names_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-treatment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caring-project",
   "metadata": {},
   "source": [
    "Loading CSV into DataFrame\n",
    "In the previous exercise, you have seen a method of creating DataFrame but generally, loading data from CSV file is the most common method of creating DataFrames. In this exercise, you'll create a PySpark DataFrame from a people.csv file that is already provided to you as a file_path and confirm the created object is a PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dress-factory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:00:53.997782Z",
     "start_time": "2021-04-13T20:00:48.137229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of people_df is <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "file_path = 'spark_bigdatafundamentals/people.csv'\n",
    "# Create an DataFrame from file_path\n",
    "people_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check the type of people_df\n",
    "print(\"The type of people_df is\", type(people_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "horizontal-canadian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:01:11.100738Z",
     "start_time": "2021-04-13T20:01:10.765082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------------+------+-------------+\n",
      "|_c0|person_id|             name|   sex|date of birth|\n",
      "+---+---------+-----------------+------+-------------+\n",
      "|  0|      100|   Penelope Lewis|female|   1990-08-31|\n",
      "|  1|      101|    David Anthony|  male|   1971-10-14|\n",
      "|  2|      102|        Ida Shipp|female|   1962-05-24|\n",
      "|  3|      103|     Joanna Moore|female|   2017-03-10|\n",
      "|  4|      104|   Lisandra Ortiz|female|   2020-08-05|\n",
      "|  5|      105|    David Simmons|  male|   1999-12-30|\n",
      "|  6|      106|    Edward Hudson|  male|   1983-05-09|\n",
      "|  7|      107|     Albert Jones|  male|   1990-09-13|\n",
      "|  8|      108| Leonard Cavender|  male|   1958-08-08|\n",
      "|  9|      109|   Everett Vadala|  male|   2005-05-24|\n",
      "| 10|      110| Freddie Claridge|  male|   2002-05-07|\n",
      "| 11|      111|Annabelle Rosseau|female|   1989-07-13|\n",
      "| 12|      112|    Eulah Emanuel|female|   1976-01-19|\n",
      "| 13|      113|       Shaun Love|  male|   1970-05-26|\n",
      "| 14|      114|Alejandro Brennan|  male|   1980-12-22|\n",
      "| 15|      115|Robert Mcreynolds|  male|   1973-12-27|\n",
      "| 16|      116|   Carla Spickard|female|   1985-06-13|\n",
      "| 17|      117|Florence Eberhart|female|   2024-06-01|\n",
      "| 18|      118|     Tina Gaskins|female|   1966-12-05|\n",
      "| 19|      119| Florence Mulhern|female|   1959-05-31|\n",
      "+---+---------+-----------------+------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dense-display",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:01:29.958244Z",
     "start_time": "2021-04-13T20:01:29.758267Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0, person_id=100, name='Penelope Lewis', sex='female', date of birth='1990-08-31'),\n",
       " Row(_c0=1, person_id=101, name='David Anthony', sex='male', date of birth='1971-10-14'),\n",
       " Row(_c0=2, person_id=102, name='Ida Shipp', sex='female', date of birth='1962-05-24'),\n",
       " Row(_c0=3, person_id=103, name='Joanna Moore', sex='female', date of birth='2017-03-10'),\n",
       " Row(_c0=4, person_id=104, name='Lisandra Ortiz', sex='female', date of birth='2020-08-05')]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "boring-essex",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:05:22.686108Z",
     "start_time": "2021-04-13T20:05:21.349056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+------+-------------+\n",
      "|_c0|person_id|            name|   sex|date of birth|\n",
      "+---+---------+----------------+------+-------------+\n",
      "|  0|      100|  Penelope Lewis|female|   1990-08-31|\n",
      "|  1|      101|   David Anthony|  male|   1971-10-14|\n",
      "|  2|      102|       Ida Shipp|female|   1962-05-24|\n",
      "|  3|      103|    Joanna Moore|female|   2017-03-10|\n",
      "|  4|      104|  Lisandra Ortiz|female|   2020-08-05|\n",
      "|  5|      105|   David Simmons|  male|   1999-12-30|\n",
      "|  6|      106|   Edward Hudson|  male|   1983-05-09|\n",
      "|  7|      107|    Albert Jones|  male|   1990-09-13|\n",
      "|  8|      108|Leonard Cavender|  male|   1958-08-08|\n",
      "|  9|      109|  Everett Vadala|  male|   2005-05-24|\n",
      "+---+---------+----------------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "There are 100000 rows in the people_df DataFrame.\n",
      "There are 5 columns in the people_df DataFrame and their names are ['_c0', 'person_id', 'name', 'sex', 'date of birth']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 observations \n",
    "people_df.show(10)\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df DataFrame.\".format(people_df.count()))\n",
    "\n",
    "# Count the number of columns and their names\n",
    "print(\"There are {} columns in the people_df DataFrame and their names are {}\".format(len(people_df.columns), people_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "elementary-train",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:06:47.027978Z",
     "start_time": "2021-04-13T20:06:43.557269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+-------------+\n",
      "|            name|   sex|date of birth|\n",
      "+----------------+------+-------------+\n",
      "|  Penelope Lewis|female|   1990-08-31|\n",
      "|   David Anthony|  male|   1971-10-14|\n",
      "|       Ida Shipp|female|   1962-05-24|\n",
      "|    Joanna Moore|female|   2017-03-10|\n",
      "|  Lisandra Ortiz|female|   2020-08-05|\n",
      "|   David Simmons|  male|   1999-12-30|\n",
      "|   Edward Hudson|  male|   1983-05-09|\n",
      "|    Albert Jones|  male|   1990-09-13|\n",
      "|Leonard Cavender|  male|   1958-08-08|\n",
      "|  Everett Vadala|  male|   2005-05-24|\n",
      "+----------------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "There were 100000 rows before removing duplicates, and 99998 rows after removing duplicates\n"
     ]
    }
   ],
   "source": [
    "# Select name, sex and date of birth columns\n",
    "people_df_sub = people_df.select('name', 'sex', 'date of birth')\n",
    "\n",
    "# Print the first 10 observations from people_df_sub\n",
    "people_df_sub.show(10)\n",
    "\n",
    "# Remove duplicate entries from people_df_sub\n",
    "people_df_sub_nodup = people_df_sub.dropDuplicates()\n",
    "\n",
    "# Count the number of rows\n",
    "print(\"There were {} rows before removing duplicates, and {} rows after removing duplicates\".format(people_df_sub.count(), people_df_sub_nodup.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "certified-hurricane",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:07:57.412594Z",
     "start_time": "2021-04-13T20:07:56.040623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 49014 rows in the people_df_female DataFrame and 49066 rows in the people_df_male DataFrame\n"
     ]
    }
   ],
   "source": [
    "# Filter people_df to select females \n",
    "people_df_female = people_df.filter(people_df.sex == \"female\")\n",
    "\n",
    "# Filter people_df to select males\n",
    "people_df_male = people_df.filter(people_df.sex == \"male\")\n",
    "\n",
    "# Count the number of rows \n",
    "print(\"There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame\".format(people_df_female.count(), people_df_male.count()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "twenty-advisory",
   "metadata": {},
   "source": [
    "Running SQL Queries Programmatically\n",
    "DataFrames can easily be manipulated using SQL queries in PySpark. The sql() function on a SparkSession enables applications to run SQL queries programmatically and returns the result as another DataFrame. In this exercise, you'll create a temporary table of the people_df DataFrame that you created previously, then construct a query to select the names of the people from the temporary table and assign the result to a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "approximate-malta",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:04:38.544564Z",
     "start_time": "2021-04-13T21:04:36.895881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            name|\n",
      "+----------------+\n",
      "|  Penelope Lewis|\n",
      "|   David Anthony|\n",
      "|       Ida Shipp|\n",
      "|    Joanna Moore|\n",
      "|  Lisandra Ortiz|\n",
      "|   David Simmons|\n",
      "|   Edward Hudson|\n",
      "|    Albert Jones|\n",
      "|Leonard Cavender|\n",
      "|  Everett Vadala|\n",
      "+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary table \"people\"\n",
    "people_df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Construct a query to select the names of the people from the temporary table \"people\"\n",
    "query = '''SELECT name FROM people'''\n",
    "\n",
    "# Assign the result of Spark's query to people_df_names\n",
    "people_df_names = spark.sql(query)\n",
    "\n",
    "# Print the top 10 names of the people\n",
    "people_df_names.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "annoying-therapist",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:09:35.427323Z",
     "start_time": "2021-04-13T21:09:34.925661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 49014 rows in the people_female_df and 49066 rows in the people_male_df DataFrames\n"
     ]
    }
   ],
   "source": [
    "# Filter the people table to select female sex \n",
    "people_female_df = spark.sql('''SELECT * FROM people WHERE sex==\"female\"''')\n",
    "\n",
    "# Filter the people table DataFrame to select male sex\n",
    "people_male_df = spark.sql('''SELECT * FROM people WHERE sex==\"male\"''')\n",
    "\n",
    "# Count the number of rows in both DataFrames\n",
    "print(\"There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames\".format(people_female_df.count(), people_male_df.count()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bored-packing",
   "metadata": {},
   "source": [
    "PySpark DataFrame visualization\n",
    "Graphical representations or visualization of data is imperative for understanding as well as interpreting the data. In this simple data visualization exercise, you'll first print the column names of names_df DataFrame that you created earlier, then convert the names_df to Pandas DataFrame and finally plot the contents as horizontal bar plot with names of the people on the x-axis and their age on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "suspended-illinois",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:16:40.420276Z",
     "start_time": "2021-04-13T21:16:40.415290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the column names of names_df\n",
    "print(\"The column names of names_df are\", names_df.columns)\n",
    "\n",
    "# Convert to Pandas DataFrame  \n",
    "df_pandas = names_df.toPandas()\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "df_pandas.plot(kind='barh', x='Name', y='Age', colormap='winter_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-context",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:16:10.998154Z",
     "start_time": "2021-04-13T21:16:00.113616Z"
    }
   },
   "source": [
    "Every 4 years, the soccer fans throughout the world celebrates a festival called “Fifa World Cup” and with that, everything seems to change in many countries. In this 3 part exercise, you'll be doing some exploratory data analysis (EDA) on the \"FIFA 2018 World Cup Player\" dataset using PySpark SQL which involve DataFrame operations, SQL queries and visualization.\n",
    "\n",
    "In the first part, you'll load FIFA 2018 World Cup Players dataset (Fifa2018_dataset.csv) which is in CSV format into a PySpark's dataFrame and inspect the data using basic DataFrame operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "complicated-variation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:20:07.585971Z",
     "start_time": "2021-04-13T21:20:06.993375Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Photo: string (nullable = true)\n",
      " |-- Nationality: string (nullable = true)\n",
      " |-- Flag: string (nullable = true)\n",
      " |-- Overall: integer (nullable = true)\n",
      " |-- Potential: integer (nullable = true)\n",
      " |-- Club: string (nullable = true)\n",
      " |-- Club Logo: string (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Wage: string (nullable = true)\n",
      " |-- Special: integer (nullable = true)\n",
      " |-- Acceleration: string (nullable = true)\n",
      " |-- Aggression: string (nullable = true)\n",
      " |-- Agility: string (nullable = true)\n",
      " |-- Balance: string (nullable = true)\n",
      " |-- Ball control: string (nullable = true)\n",
      " |-- Composure: string (nullable = true)\n",
      " |-- Crossing: string (nullable = true)\n",
      " |-- Curve: string (nullable = true)\n",
      " |-- Dribbling: string (nullable = true)\n",
      " |-- Finishing: string (nullable = true)\n",
      " |-- Free kick accuracy: string (nullable = true)\n",
      " |-- GK diving: string (nullable = true)\n",
      " |-- GK handling: string (nullable = true)\n",
      " |-- GK kicking: string (nullable = true)\n",
      " |-- GK positioning: string (nullable = true)\n",
      " |-- GK reflexes: string (nullable = true)\n",
      " |-- Heading accuracy: string (nullable = true)\n",
      " |-- Interceptions: string (nullable = true)\n",
      " |-- Jumping: string (nullable = true)\n",
      " |-- Long passing: string (nullable = true)\n",
      " |-- Long shots: string (nullable = true)\n",
      " |-- Marking: string (nullable = true)\n",
      " |-- Penalties: string (nullable = true)\n",
      " |-- Positioning: string (nullable = true)\n",
      " |-- Reactions: string (nullable = true)\n",
      " |-- Short passing: string (nullable = true)\n",
      " |-- Shot power: string (nullable = true)\n",
      " |-- Sliding tackle: string (nullable = true)\n",
      " |-- Sprint speed: string (nullable = true)\n",
      " |-- Stamina: string (nullable = true)\n",
      " |-- Standing tackle: string (nullable = true)\n",
      " |-- Strength: string (nullable = true)\n",
      " |-- Vision: string (nullable = true)\n",
      " |-- Volleys: string (nullable = true)\n",
      " |-- CAM: double (nullable = true)\n",
      " |-- CB: double (nullable = true)\n",
      " |-- CDM: double (nullable = true)\n",
      " |-- CF: double (nullable = true)\n",
      " |-- CM: double (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- LAM: double (nullable = true)\n",
      " |-- LB: double (nullable = true)\n",
      " |-- LCB: double (nullable = true)\n",
      " |-- LCM: double (nullable = true)\n",
      " |-- LDM: double (nullable = true)\n",
      " |-- LF: double (nullable = true)\n",
      " |-- LM: double (nullable = true)\n",
      " |-- LS: double (nullable = true)\n",
      " |-- LW: double (nullable = true)\n",
      " |-- LWB: double (nullable = true)\n",
      " |-- Preferred Positions: string (nullable = true)\n",
      " |-- RAM: double (nullable = true)\n",
      " |-- RB: double (nullable = true)\n",
      " |-- RCB: double (nullable = true)\n",
      " |-- RCM: double (nullable = true)\n",
      " |-- RDM: double (nullable = true)\n",
      " |-- RF: double (nullable = true)\n",
      " |-- RM: double (nullable = true)\n",
      " |-- RS: double (nullable = true)\n",
      " |-- RW: double (nullable = true)\n",
      " |-- RWB: double (nullable = true)\n",
      " |-- ST: double (nullable = true)\n",
      "\n",
      "There are 17981 rows in the fifa_df DataFrame\n"
     ]
    }
   ],
   "source": [
    "file_path = 'spark_bigdatafundamentals/Fifa2018_dataset.csv'\n",
    "# Load the Dataframe\n",
    "fifa_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Check the schema of columns\n",
    "fifa_df.printSchema()\n",
    "\n",
    "# Print the total number of rows\n",
    "print(\"There are {} rows in the fifa_df DataFrame\".format(fifa_df.count()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "civil-omega",
   "metadata": {},
   "source": [
    "Part 2: SQL Queries on DataFrame\n",
    "The fifa_df DataFrame that we created has additional information about datatypes and names of columns associated with it. This additional information allows PySpark SQL to run SQL queries on DataFrame. SQL queries are concise and easy to run compared to DataFrame operations. But in order to apply SQL queries on DataFrame first, you need to create a temporary view of DataFrame as a table and then apply SQL queries on the created table (Running SQL Queries Programmatically).\n",
    "\n",
    "In the second part, you'll create a temporary table of fifa_df DataFrame and run SQL queries to extract the 'Age' column of players from Germany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "powered-finnish",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:22:09.620311Z",
     "start_time": "2021-04-13T21:22:08.725158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              Age|\n",
      "+-------+-----------------+\n",
      "|  count|             1140|\n",
      "|   mean|24.20263157894737|\n",
      "| stddev|4.197096712293752|\n",
      "|    min|               16|\n",
      "|    max|               36|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view of fifa_df\n",
    "fifa_df.createOrReplaceTempView('fifa_df_table')\n",
    "\n",
    "# Construct the \"query\"\n",
    "query = '''SELECT Age FROM fifa_df_table WHERE Nationality == \"Germany\"'''\n",
    "\n",
    "# Apply the SQL \"query\"\n",
    "fifa_df_germany_age = spark.sql(query)\n",
    "\n",
    "# Generate basic statistics\n",
    "fifa_df_germany_age.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "excited-signature",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:23:51.148376Z",
     "start_time": "2021-04-13T21:23:50.931913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Convert fifa_df to fifa_df_germany_age_pandas DataFrame\n",
    "fifa_df_germany_age_pandas = fifa_df_germany_age.toPandas()\n",
    "\n",
    "# Plot the 'Age' density of Germany Players\n",
    "fifa_df_germany_age_pandas.plot(kind='density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-wellington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:25:34.143913Z",
     "start_time": "2021-04-13T21:25:34.137942Z"
    }
   },
   "source": [
    "[Here](https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0) is a great article for how to interpret density functions. From the link given, here is a simple explanation as to what the x and y axis represent\n",
    "\n",
    "The x-axis is the value of the variable just like in a histogram, but what exactly does the y-axis represent? The y-axis in a density plot is the probability density function for the kernel density estimation. However, we need to be careful to specify this is a probability density and not a probability. The difference is the probability density is the probability per unit on the x-axis. To convert to an actual probability, we need to find the area under the curve for a specific interval on the x-axis. Somewhat confusingly, because this is a probability density and not a probability, the y-axis can take values greater than one. The only requirement of the density plot is that the total area under the curve integrates to one. I generally tend to think of the y-axis on a density plot as a value only for relative comparisons between different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-lithuania",
   "metadata": {},
   "source": [
    "# Machine Learning with PySpark MLlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "traditional-density",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:43:00.793141Z",
     "start_time": "2021-04-18T10:43:00.618145Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the library for ALS\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "# Import the library for Logistic Regression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "# Import the library for Kmeans\n",
    "from pyspark.mllib.clustering import KMeans\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "pursuant-creature",
   "metadata": {},
   "source": [
    "Collaborative filtering is a technique for recommender systems wherein users' ratings and interactions with various products are used to recommend new ones. With the advent of Machine Learning and parallelized processing of data, Recommender systems have become widely popular in recent years, and are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags. In this 3-part exercise, your goal is to develop a simple movie recommendation system using PySpark MLlib using a subset of MovieLens 100k dataset.\n",
    "\n",
    "In the first part, you'll first load the MovieLens data (ratings.csv) into RDD and from each line in the RDD which is formatted as userId,movieId,rating,timestamp, you'll need to map the MovieLens data to a Ratings object (userID, productID, rating) after removing timestamp column and finally you'll split the RDD into training and test RDDs.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace. Also file_path variable (which is the path to the ratings.csv file), and ALS class are already available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "distinguished-villa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:43:01.083567Z",
     "start_time": "2021-04-18T10:43:01.048663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data into RDD\n",
    "file_path = 'spark_bigdatafundamentals/ratings.csv'\n",
    "data = sc.textFile(file_path)\n",
    "\n",
    "# Split the RDD \n",
    "ratings = data.map(lambda l: l.split(','))\n",
    "\n",
    "# Transform the ratings RDD\n",
    "ratings_final = ratings.map(lambda line: (int(line[0]), int(line[1]), float(line[2])))\n",
    "\n",
    "# Split the data into training and test\n",
    "training_data, test_data = ratings_final.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "average-mauritius",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T18:24:55.486126Z",
     "start_time": "2021-04-17T18:24:55.472122Z"
    }
   },
   "source": [
    "After splitting the data into training and test data, in the second part of the exercise, you'll train the ALS algorithm using the training data. PySpark MLlib's ALS algorithm has the following mandatory parameters - rank (the number of latent factors in the model) and iterations (number of iterations to run). After training the ALS model, you can use the model to predict the ratings from the test data. For this, you will provide the user and item columns from the test dataset and finally print the first 2 rows of predictAll() output.\n",
    "\n",
    "Remember, you have SparkContext sc, training_data and test_data are already available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "loaded-valley",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:43:11.932269Z",
     "start_time": "2021-04-18T10:43:01.783401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=311, product=45208, rating=0.9598439700716949),\n",
       " Rating(user=468, product=6400, rating=1.1477115284864197)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the ALS model on the training data\n",
    "model = ALS.train(training_data, rank=10, iterations=2)\n",
    "\n",
    "# Drop the ratings column \n",
    "testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# Predict the model  \n",
    "predictions = model.predictAll(testdata_no_rating)\n",
    "\n",
    "# Print the first rows of the RDD\n",
    "predictions.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-phenomenon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T20:32:47.101831Z",
     "start_time": "2021-04-17T20:32:45.643122Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "damaged-passage",
   "metadata": {},
   "source": [
    "After generating the predicted ratings from the test data using ALS model, in this final part of the exercise, you'll prepare the data for calculating Mean Square Error (MSE) of the model. The MSE is the average value of (original rating – predicted rating)^2 for all users and indicates the absolute fit of the model to the data. To do this, first, you'll organize both the ratings and prediction RDDs to make a tuple of ((user, product), rating)), then join the ratings RDD with prediction RDD and finally apply a squared difference function along with mean() to get the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acute-supervision",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:43:38.061640Z",
     "start_time": "2021-04-18T10:43:20.720065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of the model for the test data = 1.75\n"
     ]
    }
   ],
   "source": [
    "# Prepare ratings data\n",
    "rates = ratings_final.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "# Prepare predictions data\n",
    "preds = predictions.map(lambda r: ((r[0], r[1]),r[2]))\n",
    "\n",
    "# Join the ratings data with predictions data\n",
    "rates_and_preds = rates.join(preds)\n",
    "\n",
    "# Calculate and print MSE\n",
    "MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error of the model for the test data = {:.2f}\".format(MSE))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "common-curtis",
   "metadata": {},
   "source": [
    "Loading spam and non-spam data\n",
    "Logistic Regression is a popular method to predict a categorical response. Probably one of the most common applications of the logistic regression is the message or email spam classification. In this 3-part exercise, you'll create an email spam classifier with logistic regression using Spark MLlib. Here are the brief steps for creating a spam classifier.\n",
    "\n",
    "Create an RDD of strings representing email.\n",
    "Run MLlib’s feature extraction algorithms to convert text into an RDD of vectors.\n",
    "Call a classification algorithm on the RDD of vectors to return a model object to classify new points.\n",
    "Evaluate the model on a test dataset using one of MLlib’s evaluation functions.\n",
    "In the first part of the exercise, you'll load the 'spam' and 'ham' (non-spam) files into RDDs, split the emails into individual words and look at the first element in each of the RDD.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace. Also file_path_spam variable (which is the path to the 'spam' file) and file_path_non_spam (which is the path to the 'non-spam' file) is already available in your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "outer-pillow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:43:40.761346Z",
     "start_time": "2021-04-18T10:43:40.756360Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path_spam = 'spark_bigdatafundamentals/spam.txt'\n",
    "file_path_non_spam = 'spark_bigdatafundamentals/ham.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sitting-violin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:43:42.483320Z",
     "start_time": "2021-04-18T10:43:41.311746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element in spam_words is You\n",
      "The first element in non_spam_words is Rofl.\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets into RDDs\n",
    "spam_rdd = sc.textFile(file_path_spam)\n",
    "non_spam_rdd = sc.textFile(file_path_non_spam)\n",
    "\n",
    "# Split the email messages into words\n",
    "spam_words = spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "non_spam_words = non_spam_rdd.flatMap(lambda email: email.split(' '))\n",
    "\n",
    "# Print the first element in the split RDD\n",
    "print(\"The first element in spam_words is\", spam_words.first())\n",
    "print(\"The first element in non_spam_words is\", non_spam_words.first())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "connected-writing",
   "metadata": {},
   "source": [
    "After splitting the emails into words, our raw data set of 'spam' and 'non-spam' is currently composed of 1-line messages consisting of spam and non-spam messages. In order to classify these messages, we need to convert text into features.\n",
    "\n",
    "In the second part of the exercise, you'll first create a HashingTF() instance to map text to vectors of 200 features, then for each message in 'spam' and 'non-spam' files you'll split them into words, and each word is mapped to one feature. These are the features that will be used to decide whether a message is 'spam' or 'non-spam'. Next, you'll create labels for features. For a valid message, the label will be 0 (i.e. the message is not spam) and for a 'spam' message, the label will be 1 (i.e. the message is spam). Finally, you'll combine both the labeled datasets.\n",
    "\n",
    "Remember, you have a SparkContext sc available in your workspace. Also spam_words and non_spam_words variables are already available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "isolated-deployment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:43:43.510276Z",
     "start_time": "2021-04-18T10:43:43.463400Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Create a HashingTf instance with 200 features\n",
    "tf = HashingTF(numFeatures=200)\n",
    "\n",
    "# Map each word to one feature\n",
    "spam_features = tf.transform(spam_words)\n",
    "non_spam_features = tf.transform(non_spam_words)\n",
    "\n",
    "# Label the features: 1 for spam, 0 for non-spam\n",
    "spam_samples = spam_features.map(lambda features:LabeledPoint(1, features))\n",
    "non_spam_samples = non_spam_features.map(lambda features:LabeledPoint(0, features))\n",
    "\n",
    "# Combine the two datasets\n",
    "samples = spam_samples.join(non_spam_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-austin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T21:10:18.349112Z",
     "start_time": "2021-04-17T21:10:15.610428Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "breathing-humidity",
   "metadata": {},
   "source": [
    "\n",
    "After creating labels and features for the data, we’re ready to build a model that can learn from it (training). But before you train the model, you'll split the combined dataset into training and testing dataset because it can assign a probability of being spam to each data point. We can then decide to classify messages as spam or not, depending on how high the probability.\n",
    "\n",
    "In this final part of the exercise, you'll split the data into training and test, run Logistic Regression on the training data, apply the same HashingTF() feature transformation to get vectors on a positive example (spam) and a negative one (non-spam) and finally check the accuracy of the model trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-music",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:21:50.917252Z",
     "start_time": "2021-04-18T10:21:44.965615Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acting-standing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:44:41.867696Z",
     "start_time": "2021-04-18T10:44:41.863705Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "train_samples,test_samples = samples.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegressionWithLBFGS.train(train_samples)\n",
    "\n",
    "# Create a prediction label from the test data\n",
    "predictions = model.predict(test_samples.map(lambda x: x.features))\n",
    "\n",
    "# Combine original labels with the predicted labels\n",
    "labels_and_preds = test_samples.map(lambda x: x.label).zip(predictions)\n",
    "# Check the accuracy of the model on the test data\n",
    "accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_samples.count())\n",
    "print(\"Model accuracy : {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-thinking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "centered-restaurant",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "iraqi-accountability",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:44:47.891058Z",
     "start_time": "2021-04-18T10:44:46.781615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5000 rows in the rdd_split_int dataset\n"
     ]
    }
   ],
   "source": [
    "file_path = 'spark_bigdatafundamentals/5000_points.txt'\n",
    "# Load the dataset into a RDD\n",
    "clusterRDD = sc.textFile(file_path)\n",
    "\n",
    "# Split the RDD based on tab\n",
    "rdd_split = clusterRDD.map(lambda x: x.split(\"\\t\"))\n",
    "\n",
    "# Transform the split RDD by creating a list of integers\n",
    "rdd_split_int = rdd_split.map(lambda x: [int(x[0]), int(x[1])])\n",
    "\n",
    "# Count the number of rows in RDD \n",
    "print(\"There are {} rows in the rdd_split_int dataset\".format(rdd_split_int.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "induced-participation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T10:45:01.808305Z",
     "start_time": "2021-04-18T10:45:01.805274Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans\n",
    "# Train the model with clusters from 13 to 16 and compute WSSSE \n",
    "for clst in range(13, 17):\n",
    "    model = KMeans.train(rdd_split_int,k= clst, seed=1)\n",
    "    WSSSE = rdd_split_int.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "    print(\"The cluster {} has Within Set Sum of Squared Error {}\".format(clst, WSSSE))\n",
    "\n",
    "# Train the model again with the best k \n",
    "model = KMeans.train(rdd_split_int, k=15, seed=1)\n",
    "\n",
    "# Get cluster centers\n",
    "cluster_centers = model.clusterCenters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rdd_split_int RDD into Spark DataFrame\n",
    "rdd_split_int_df = spark.createDataFrame(rdd_split_int, schema=[\"col1\", \"col2\"])\n",
    "\n",
    "# Convert Spark DataFrame into Pandas DataFrame\n",
    "rdd_split_int_df_pandas = rdd_split_int_df.toPandas()\n",
    "\n",
    "# Convert \"cluster_centers\" that you generated earlier into Pandas DataFrame\n",
    "cluster_centers_pandas = pd.DataFrame(cluster_centers, columns=[\"col1\", \"col2\"])\n",
    "\n",
    "# Create an overlaid scatter plot\n",
    "plt.scatter(rdd_split_int_df_pandas[\"col1\"], rdd_split_int_df_pandas[\"col2\"])\n",
    "plt.scatter(cluster_centers_pandas[\"col1\"], cluster_centers_pandas[\"col2\"], color=\"red\", marker=\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-overall",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-monster",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
